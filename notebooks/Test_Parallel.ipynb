{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59299/2777325278.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "from utils.utils import partition_data\n",
    "from utils.feature_selection import *\n",
    "#from plot import *\n",
    "#from simulation_setup import *\n",
    "from models import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI_parallel import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()\n",
    "path=os.path.dirname(path)\n",
    "path_real=os.path.join(path,'data','real')\n",
    "path_syn=os.path.join(path,'data','synthetic')\n",
    "mat_files_real=glob(os.path.join(path_real, '*.mat'))\n",
    "mat_file_names_real={os.path.basename(x).split('.')[0]:x for x in mat_files_real}\n",
    "files_syn=glob(os.path.join(path_syn, '*.pkl'))\n",
    "file_names_syn={os.path.basename(x).split('.')[0]:x for x in files_syn}\n",
    "csv_files=glob(os.path.join(path, '*.csv'))\n",
    "csv_file_names={os.path.basename(x).split('.')[0]:x for x in csv_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_syn(filename,path):\n",
    "    file_to_read = open(os.path.join(path,filename), \"rb\")\n",
    "    dict = pickle.load(file_to_read)\n",
    "    data=[]\n",
    "    for k in dict.keys():\n",
    "        data.append(dict[k])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(os.path.join(path_syn,'ball_6_dim.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "file_to_read = open(os.path.join(path_syn,'anomalies.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_xaxis,X_yaxis,X_bisect,X_bisect_3d,X_bisect_6d=loaded_dictionary['X_xaxis'],loaded_dictionary['X_yaxis'],loaded_dictionary['X_bisec'],loaded_dictionary['X_bisec_3d'],loaded_dictionary['X_bisec_6d']\n",
    "file_to_read = open(os.path.join(path_syn,'toy_datasets.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_toy_2d,X_toy_3d,X_toy_4d,X_toy_6d,y = loaded_dictionary['X_toy_2d'],loaded_dictionary['X_toy_3d'],loaded_dictionary['X_toy_4d'],loaded_dictionary['X_toy_6d'],loaded_dictionary['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data=loadmat(mat_file_names_real[filename])\n",
    "    X,y=data['X'],data['y']\n",
    "    y=np.hstack(y)\n",
    "    return X,y \n",
    "\n",
    "def load_data_csv(filename):\n",
    "    data=pd.read_csv(csv_file_names[filename])\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data=data.drop(columns=['Unnamed: 0'])\n",
    "    X=data[data.columns[data.columns!='Target']]\n",
    "    y=data['Target']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imps(model,X_train,X_test,n_runs,name,pwd,dim,f=6):\n",
    "\n",
    "    name='GFI_'+name\n",
    "\n",
    "    #X_test=np.r_[X_train,X_test]\n",
    "\n",
    "    imps=np.zeros(shape=(n_runs,X_train.shape[1]))\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        model.fit(X_train)\n",
    "        imps[i,:]=model.Global_importance(X_test,calculate=True,overwrite=False,depth_based=False)\n",
    "\n",
    "    path = pwd + '/results/imp/imp_score_' + name + '.pkl'\n",
    "    with open(path, 'wb') as fl:\n",
    "        pickle.dump(imps,fl)\n",
    "\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation_plots(name,pwd):\n",
    "\n",
    "    os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\data')\n",
    "    if name=='diabetes' or name=='moodify':\n",
    "        X,y=csv_dataset(name,os.getcwd()+'\\\\')\n",
    "    else:\n",
    "        X,y=dataset(name,os.getcwd()+'\\\\')\n",
    "\n",
    "    X,y=downsample(X,y)\n",
    "    X_train,X_test=partition_data(X,y)\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    y_train=np.zeros(X_train.shape[0])\n",
    "    y_test=np.ones(X_test.shape[0])\n",
    "    y=np.concatenate([y_train,y_test])\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "    scaler2=StandardScaler()\n",
    "    X=scaler2.fit_transform(X)\n",
    "    EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "    dim=X.shape[1]\n",
    "\n",
    "    # No Split \n",
    "\n",
    "    imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "\n",
    "    path = pwd + '\\\\results\\\\davide\\\\Importance_Scores\\\\Imp Score\\\\imp_score_GFI_' + name + '.pkl'\n",
    "    with open(path, 'rb') as fl:\n",
    "        imps = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129, 13), (129,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='wine'\n",
    "X,y=load_data(name)\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119, 13), (10, 13))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1,seed=126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 35.46605706214905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)\n",
    "EDIFFI.set_num_processes(8,8)\n",
    "\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "imps=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((351, 33), (351,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='ionosphere'\n",
    "X,y=load_data(name)\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:54<00:00, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 174.5880789756775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:29<00:00,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 89.17974472045898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)\n",
    "EDIFFI.set_num_processes(12)\n",
    "\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Ionosphere\n",
    "\n",
    "Serial -> 174.5880789756775\n",
    "\n",
    "8 threads\n",
    "\n",
    "Parallel -> 147.8202040195465\n",
    "\n",
    "12 threads\n",
    "\n",
    "Parallel -> 137.81197714805603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDIFFI.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (100000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y=load_data_csv('diabetes')\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=np.load('test_stat_parallel.npz',allow_pickle=True)\n",
    "data_parallel=stats['importances_matrix'].tolist()\n",
    "time_data_parallel=stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.36362202167511, 'std': 0.23129910849918273}, dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parallel.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=np.load('test_stat_serial.npz',allow_pickle=True)\n",
    "data_serial=stats['importances_matrix'].tolist()\n",
    "time_data_serial=stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.5636572360992433, 'std': 0.5714168745774968},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_serial.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if `data_parallel` and `data_serial` are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.995204332975845e-14\n",
      "1.532107773982716e-13\n",
      "-3.4638958368304884e-13\n",
      "7.327471962526033e-14\n",
      "5.306866057708248e-13\n",
      "-3.774758283725532e-13\n",
      "-3.197442310920451e-13\n",
      "1.3522516439934407e-12\n",
      "-1.2434497875801753e-13\n",
      "-1.0769163338864018e-12\n"
     ]
    }
   ],
   "source": [
    "for k in data_serial.keys():\n",
    "    print(np.sum(data_serial[k]-data_parallel[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Host capri\n",
    "\n",
    "    Hostname capri.dei.unipd.it\n",
    "    User p1026u27"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
