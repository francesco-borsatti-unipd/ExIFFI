{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Host capri Davide\n",
    "\n",
    "    Hostname capri.dei.unipd.it\n",
    "    User p1026u27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "from utils.utils import partition_data\n",
    "from utils.feature_selection import *\n",
    "#from plot import *\n",
    "#from simulation_setup import *\n",
    "from models import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI_parallel import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path = os.path.dirname(path)\n",
    "path_real = os.path.join(path, \"data\", \"real\")\n",
    "mat_files_real = glob(os.path.join(path_real, \"*.mat\"))\n",
    "mat_file_names_real = {os.path.basename(x).split(\".\")[0]: x for x in mat_files_real}\n",
    "csv_files_real = glob(os.path.join(path_real, \"*.csv\"))\n",
    "csv_file_names_real = {os.path.basename(x).split(\".\")[0]: x for x in csv_files_real}\n",
    "dataset_names = list(mat_file_names_real.keys()) + list(csv_file_names_real.keys())\n",
    "mat_file_names_real.update(csv_file_names_real)\n",
    "dataset_paths = mat_file_names_real.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates from the loaded dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(X, y):\n",
    "    S = np.c_[X, y]\n",
    "    S = pd.DataFrame(S).drop_duplicates().to_numpy()\n",
    "    X, y = S[:, :-1], S[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset coming from a `.mat` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = loadmat(path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    y = np.hstack(y)\n",
    "    X, y = drop_duplicates(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset coming from a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_csv(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "    if \"Unnamed: 0\" in data.columns:\n",
    "        data = data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    X = data[data.columns[data.columns != \"Target\"]]\n",
    "    y = data[\"Target\"]\n",
    "\n",
    "    X, y = drop_duplicates(X, y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data (with `load_data` or with `load_data_csv`), scale the data and split it into train and test set obtaining `X_train`, `X_test` that will be passed to `compute_imps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(path):\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    if extension == \".csv\":\n",
    "        X, y = load_data_csv(path)\n",
    "    elif extension == \".mat\":\n",
    "        X, y = load_data(path)\n",
    "    else:\n",
    "        raise ValueError(\"Extension not supported\")\n",
    "\n",
    "    X_train, X_test = partition_data(X, y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = np.r_[X_train, X_test]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Global Importance of a given dataset `n_runs` times. At the end a matrix with shape `(n_runs, n_features)` is returned. Each row contains the global importance of the features for a given run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imps(model, X_train, X_test, n_runs):\n",
    "\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "\n",
    "    imps = np.zeros(shape=(n_runs, X_train.shape[1]))\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        model.fit(X_train)\n",
    "        imps[i, :] = model.Global_importance(\n",
    "            X_test, calculate=True, overwrite=False, depth_based=False\n",
    "        )\n",
    "\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test_exiffi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function called in the `main` of `test_parallel.py` used to do the experiments on the CAPRI HPC server. For a given set of datasets it computes the global importance `n_runs` times using `Extended_DIFFI_parallel` or `Extended_DIFFI_original`and saves the importances matrices, the time stats obtained and the test arguments in a `.npz` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `test_exiffi` Parameters\n",
    "\n",
    "- `X_train`: the train set\n",
    "- `X_test`: the test set\n",
    "- `savedir`: directory where to save the results in `.npz` format\n",
    "- `n_runs`: number of runs to do\n",
    "- `seed`: random seed to obtain reproducibile results and compare the importances matrices obtaind from the parallel and the serial version of the algorithm (they must be the same to certify the correctness of the parallel version)\n",
    "- `parallel`: Boolean variable used to choose between the parallel and the serial version of the algorithm\n",
    "- `n_cores`: Number of threads to use in the parallel version of the algorithm. This coincides with the number of cores set with the `--cpus-per-task` options in the `.job` file\n",
    "- `num_trees`: Number of trees used by ExIFFI. The higher the more complex and more computationally expensive the algorithm is\n",
    "- `name`: Name of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_exiffi(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    savedir,\n",
    "    n_runs=10,\n",
    "    seed=None,\n",
    "    parallel=False,\n",
    "    n_cores=2,\n",
    "    num_trees=300,\n",
    "    name=\"\",\n",
    "):\n",
    "    args_to_avoid = [\"X_train\", \"X_test\", \"savedir\", \"args_to_avoid\", \"args\"]\n",
    "    args = dict()\n",
    "    for k, v in locals().items():\n",
    "        if k in args_to_avoid:\n",
    "            continue\n",
    "        args[k] = v\n",
    "\n",
    "    ex_time = []\n",
    "    ex_imps = {}\n",
    "\n",
    "    for i in trange(n_runs):\n",
    "        seed = None if seed is None else seed + i\n",
    "\n",
    "        if parallel:\n",
    "            EDIFFI = Extended_DIFFI_parallel(\n",
    "                n_trees=num_trees, max_depth=100, subsample_size=256, plus=1, seed=seed\n",
    "            )\n",
    "            EDIFFI.set_num_processes(n_cores, n_cores)\n",
    "        else:\n",
    "            EDIFFI = Extended_DIFFI_original(\n",
    "                n_trees=num_trees, max_depth=100, subsample_size=256, plus=1, seed=seed\n",
    "            )\n",
    "\n",
    "        start = time.time()\n",
    "        imps = compute_imps(EDIFFI, X_train, X_test, 10)\n",
    "        ex_imps[\"Execution \" + str(i)] = imps\n",
    "        end = time.time()\n",
    "        ex_time.append(end - start)\n",
    "\n",
    "    # print(ex_imps)\n",
    "    time_stat = {\"mean\": np.mean(ex_time), \"std\": np.std(ex_time)}\n",
    "    filename = \"test_stat_parallel.npz\" if parallel else \"test_stat_serial.npz\"\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%d-%m-%Y_%H-%M-%S\", t)\n",
    "    filename = current_time + \"_\" + name + \"_\" + filename\n",
    "\n",
    "    # if dir does not exist, create it\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    filepath = os.path.join(savedir, filename)\n",
    "\n",
    "    np.savez(\n",
    "        filepath,\n",
    "        execution_time_stat=time_stat,\n",
    "        importances_matrix=ex_imps,\n",
    "        arguments=args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129, 13), (129,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='wine'\n",
    "X,y=load_data(dataset_paths[name])\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=False,\n",
    "    n_cores=12,\n",
    "    num_trees=10,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=True,\n",
    "    n_cores=12,\n",
    "    num_trees=200,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((350, 33), (350,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='ionosphere'\n",
    "X,y=load_data(dataset_paths[name])\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=False,\n",
    "    n_cores=12,\n",
    "    num_trees=10,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=True,\n",
    "    n_cores=12,\n",
    "    num_trees=200,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moodify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((276260, 11), (276260,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='moodify'\n",
    "X,y=load_data_csv(dataset_paths[name])\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=np.load('26-01-2024_17-42-35_test_stat_parallel_7000.npz',allow_pickle=True)\n",
    "data_parallel=stats['importances_matrix'].tolist()\n",
    "time_data_parallel=stats['execution_time_stat']\n",
    "arguments_parallel=stats['arguments'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'X', 'n_runs', 'seed', 'parallel', 'n_cores'])\n",
      "n_runs 2\n",
      "seed None\n",
      "parallel True\n",
      "n_cores 8\n"
     ]
    }
   ],
   "source": [
    "print(arguments_parallel.keys())\n",
    "\n",
    "args_to_avoid = [\"X_train\", \"X_test\", \"X\"]\n",
    "for key in arguments_parallel.keys():\n",
    "    if key not in args_to_avoid:\n",
    "        print(key,arguments_parallel[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.36362202167511, 'std': 0.23129910849918273}, dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parallel.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_stat_serial.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_stat_serial.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_serial\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportances_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m time_data_serial\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time_stat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_stat_serial.npz'"
     ]
    }
   ],
   "source": [
    "stats=np.load('test_stat_serial.npz',allow_pickle=True)\n",
    "data_serial=stats['importances_matrix'].tolist()\n",
    "time_data_serial=stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.5636572360992433, 'std': 0.5714168745774968},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_serial.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if `data_parallel` and `data_serial` are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.995204332975845e-14\n",
      "1.532107773982716e-13\n",
      "-3.4638958368304884e-13\n",
      "7.327471962526033e-14\n",
      "5.306866057708248e-13\n",
      "-3.774758283725532e-13\n",
      "-3.197442310920451e-13\n",
      "1.3522516439934407e-12\n",
      "-1.2434497875801753e-13\n",
      "-1.0769163338864018e-12\n"
     ]
    }
   ],
   "source": [
    "for k in data_serial.keys():\n",
    "    print(np.sum(data_serial[k]-data_parallel[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Thyroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 89.01771640777588, 'std': 0.0}, dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_runs': 1,\n",
       " 'seed': 120,\n",
       " 'parallel': True,\n",
       " 'n_cores': 2,\n",
       " 'num_trees': 10,\n",
       " 'name': 'annthyroid',\n",
       " 'args_to_avoid': ['X_train', 'X_test', 'savedir'],\n",
       " 'args': {...}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_load = (\n",
    "    \"../capri_code/results/npz/28-01-2024_17-45-18_annthyroid_test_stat_parallel.npz\"\n",
    ")\n",
    "\n",
    "stats = np.load(path_to_load, allow_pickle=True)\n",
    "\n",
    "display(stats['execution_time_stat'])\n",
    "display(stats['arguments'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All results\n",
    "\n",
    "We use the script `process_results.py` to read the stats of the experiments from the `.npz` files and display them on a `pd.DataFrame` that can be saved as a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "\n",
    "from capri_code.process_results import load_stats, display_stats, compute_cpu_efficiency\n",
    "\n",
    "\n",
    "results_dirpath = \"../../container/job4/results/\"\n",
    "\n",
    "stats = load_stats(results_dirpath, use_pkl=True)\n",
    "\n",
    "for i, row in stats.iterrows():\n",
    "    n_cores = max([row[\"n_cores_fit\"], row[\"n_cores_importance\"], row[\"n_cores_anomaly\"]])\n",
    "    stats.loc[i, \"cpu_efficiency\"] = compute_cpu_efficiency(row[\"real_time\"], row[\"user_time\"], n_cores)\n",
    "    \n",
    "\n",
    "# display_stats(stats)\n",
    "display_stats(stats.groupby(\"parallel\").get_group(True))\n",
    "display_stats(stats.groupby(\"parallel\").get_group(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         samp*feat\n",
      "=========================\n",
      "Moodify___________3038860\n",
      "Shuttle____________441873\n",
      "Diabetes___________343664\n",
      "Pendigits__________109920\n",
      "Annthyroid__________43200\n",
      "Cardio______________38451\n",
      "Ionosphere__________11583\n",
      "Xaxis________________6600\n",
      "Yaxis________________6600\n",
      "Bisect_______________6600\n",
      "Bisec3D______________6600\n",
      "Bisec6D______________6600\n",
      "Breastw______________6147\n",
      "Pima_________________6144\n",
      "Glass________________1917\n",
      "Wine_________________1677\n",
      "Bimodal_______________800\n"
     ]
    }
   ],
   "source": [
    "name_samples_features = [\n",
    "    (\"Bimodal\", 400, 2),\n",
    "    (\"Xaxis\", 1100, 6),\n",
    "    (\"Yaxis\", 1100, 6),\n",
    "    (\"Bisect\", 1100, 6),\n",
    "    (\"Bisec3D\", 1100, 6),\n",
    "    (\"Bisec6D\", 1100, 6),\n",
    "    (\"Annthyroid\", 7200, 6),\n",
    "    (\"Breastw\", 683, 9),\n",
    "    (\"Cardio\", 1831, 21),\n",
    "    (\"Glass\", 213, 9),\n",
    "    (\"Ionosphere\", 351, 33),\n",
    "    (\"Pendigits\", 6870, 16),\n",
    "    (\"Pima\", 768, 8),\n",
    "    (\"Shuttle\", 49097, 9),\n",
    "    (\"Wine\", 129, 13),\n",
    "    (\"Diabetes\", 85916, 4),\n",
    "    (\"Moodify\", 276260, 11),\n",
    "]\n",
    "\n",
    "size = [(n, s * f) for n, s, f in name_samples_features]\n",
    "size.sort(key=lambda x: x[1], reverse=True)  # sort by size\n",
    "\n",
    "print(f\"{'Dataset':<15}{'samp*feat':>10}\\n{'':=>25}\")\n",
    "for n, s in size:\n",
    "    print(f\"{n:_<15}{s:_>10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
