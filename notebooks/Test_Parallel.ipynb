{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "from utils.utils import partition_data\n",
    "from utils.feature_selection import *\n",
    "#from plot import *\n",
    "#from simulation_setup import *\n",
    "from models import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI_parallel import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()\n",
    "path=os.path.dirname(path)\n",
    "path_real=os.path.join(path,'data','real')\n",
    "path_syn=os.path.join(path,'data','synthetic')\n",
    "mat_files_real=glob(os.path.join(path_real, '*.mat'))\n",
    "mat_file_names_real={os.path.basename(x).split('.')[0]:x for x in mat_files_real}\n",
    "files_syn=glob(os.path.join(path_syn, '*.pkl'))\n",
    "file_names_syn={os.path.basename(x).split('.')[0]:x for x in files_syn}\n",
    "csv_files=glob(os.path.join(path, '*.csv'))\n",
    "csv_file_names={os.path.basename(x).split('.')[0]:x for x in csv_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_syn(filename,path):\n",
    "    file_to_read = open(os.path.join(path,filename), \"rb\")\n",
    "    dict = pickle.load(file_to_read)\n",
    "    data=[]\n",
    "    for k in dict.keys():\n",
    "        data.append(dict[k])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(os.path.join(path_syn,'ball_6_dim.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "file_to_read = open(os.path.join(path_syn,'anomalies.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_xaxis,X_yaxis,X_bisect,X_bisect_3d,X_bisect_6d=loaded_dictionary['X_xaxis'],loaded_dictionary['X_yaxis'],loaded_dictionary['X_bisec'],loaded_dictionary['X_bisec_3d'],loaded_dictionary['X_bisec_6d']\n",
    "file_to_read = open(os.path.join(path_syn,'toy_datasets.pkl'), \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_toy_2d,X_toy_3d,X_toy_4d,X_toy_6d,y = loaded_dictionary['X_toy_2d'],loaded_dictionary['X_toy_3d'],loaded_dictionary['X_toy_4d'],loaded_dictionary['X_toy_6d'],loaded_dictionary['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data=loadmat(mat_file_names_real[filename])\n",
    "    X,y=data['X'],data['y']\n",
    "    y=np.hstack(y)\n",
    "    return X,y \n",
    "\n",
    "def load_data_csv(filename):\n",
    "    data=pd.read_csv(csv_file_names[filename])\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data=data.drop(columns=['Unnamed: 0'])\n",
    "    X=data[data.columns[data.columns!='Target']]\n",
    "    y=data['Target']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imps(model,X_train,X_test,n_runs,name,pwd,dim,f=6):\n",
    "\n",
    "    name='GFI_'+name\n",
    "\n",
    "    #X_test=np.r_[X_train,X_test]\n",
    "\n",
    "    imps=np.zeros(shape=(n_runs,X_train.shape[1]))\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        model.fit(X_train)\n",
    "        imps[i,:]=model.Global_importance(X_test,calculate=True,overwrite=False,depth_based=False)\n",
    "\n",
    "    path = pwd + '/results/imp/imp_score_' + name + '.pkl'\n",
    "    with open(path, 'wb') as fl:\n",
    "        pickle.dump(imps,fl)\n",
    "\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation_plots(name,pwd):\n",
    "\n",
    "    os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\data')\n",
    "    if name=='diabetes' or name=='moodify':\n",
    "        X,y=csv_dataset(name,os.getcwd()+'\\\\')\n",
    "    else:\n",
    "        X,y=dataset(name,os.getcwd()+'\\\\')\n",
    "\n",
    "    X,y=downsample(X,y)\n",
    "    X_train,X_test=partition_data(X,y)\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    y_train=np.zeros(X_train.shape[0])\n",
    "    y_test=np.ones(X_test.shape[0])\n",
    "    y=np.concatenate([y_train,y_test])\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "    scaler2=StandardScaler()\n",
    "    X=scaler2.fit_transform(X)\n",
    "    EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "    dim=X.shape[1]\n",
    "\n",
    "    # No Split \n",
    "\n",
    "    imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "\n",
    "    path = pwd + '\\\\results\\\\davide\\\\Importance_Scores\\\\Imp Score\\\\imp_score_GFI_' + name + '.pkl'\n",
    "    with open(path, 'rb') as fl:\n",
    "        imps = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129, 13), (129,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='wine'\n",
    "X,y=load_data(name)\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119, 13), (10, 13))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1,seed=126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)\n",
    "EDIFFI.set_num_processes(8)\n",
    "\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((351, 33), (351,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='ionosphere'\n",
    "X,y=load_data(name)\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:54<00:00, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 174.5880789756775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:29<00:00,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 89.17974472045898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)\n",
    "EDIFFI.set_num_processes(12)\n",
    "\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Ionosphere\n",
    "\n",
    "Serial -> 174.5880789756775\n",
    "\n",
    "8 threads\n",
    "\n",
    "Parallel -> 147.8202040195465\n",
    "\n",
    "12 threads\n",
    "\n",
    "Parallel -> 137.81197714805603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDIFFI.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (100000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y=load_data_csv('diabetes')\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=np.load('test_stat.npz',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.6890564918518067, 'std': 0.5207048001277851},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'Execution 0': array([[3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ],\n",
       "       [3.44755852, 4.13613304, 3.37304616, 3.90623504, 3.63012237,\n",
       "        3.30479825, 3.527404  , 4.06928892, 3.49031055, 3.56519344,\n",
       "        3.38879328, 3.45615214, 3.9789097 ]])}, dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats['importances_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
