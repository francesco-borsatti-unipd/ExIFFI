{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "from utils.utils import partition_data\n",
    "from utils.feature_selection import *\n",
    "#from plot import *\n",
    "#from simulation_setup import *\n",
    "from models import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI_parallel import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()\n",
    "path=os.path.dirname(path)\n",
    "path=os.path.join(path,'data')\n",
    "mat_files=glob(os.path.join(path, '*.mat'))\n",
    "mat_file_names={os.path.basename(x).split('.')[0]:x for x in mat_files}\n",
    "csv_files=glob(os.path.join(path, '*.csv'))\n",
    "csv_file_names={os.path.basename(x).split('.')[0]:x for x in csv_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data=loadmat(mat_file_names[filename])\n",
    "    X,y=data['X'],data['y']\n",
    "    y=np.hstack(y)\n",
    "    return X,y \n",
    "\n",
    "def load_data_csv(filename):\n",
    "    data=pd.read_csv(csv_file_names[filename])\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data=data.drop(columns=['Unnamed: 0'])\n",
    "    X=data[data.columns[data.columns!='Target']]\n",
    "    y=data['Target']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imps(model,X_train,X_test,n_runs,name,pwd,dim,f=6):\n",
    "\n",
    "    name='GFI_'+name\n",
    "\n",
    "    #X_test=np.r_[X_train,X_test]\n",
    "\n",
    "    imps=np.zeros(shape=(n_runs,X_train.shape[1]))\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        model.fit(X_train)\n",
    "        imps[i,:]=model.Global_importance(X_test,calculate=True,overwrite=False,depth_based=False)\n",
    "\n",
    "    path = pwd + '/results/imp/imp_score_' + name + '.pkl'\n",
    "    with open(path, 'wb') as fl:\n",
    "        pickle.dump(imps,fl)\n",
    "\n",
    "    #Take the mean feature importance scores over the different runs for the Feature Importance Plot\n",
    "    #and put it in decreasing order of importance\n",
    "    mean_imp=np.mean(imps,axis=0)\n",
    "    std_imp=np.std(imps,axis=0)\n",
    "    mean_imp_val=np.sort(mean_imp)\n",
    "    feat_order=mean_imp.argsort()\n",
    "\n",
    "    plt_data={'Importances': mean_imp_val,\n",
    "              'feat_order': feat_order,\n",
    "              'std': std_imp[mean_imp.argsort()]}\n",
    "\n",
    "    path = pwd + '/results/plt_data/plt_data_' + name + '.pkl'\n",
    "    with open(path, 'wb') as fl:\n",
    "        pickle.dump(plt_data,fl)\n",
    "\n",
    "    return imps,plt_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129, 13), (129,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='wine'\n",
    "X,y=load_data(name)\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119, 13), (10, 13))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 63.42638850212097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_original(300,max_depth=100,subsample_size=256,plus=1)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m pwd\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m     16\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m imps,plt_data\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_imps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEDIFFI\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m end\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mcompute_imps\u001b[0;34m(model, X_train, X_test, n_runs, name, pwd, dim, f)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_runs)):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[0;32m---> 10\u001b[0m     imps[i,:]\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobal_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcalculate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdepth_based\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m path \u001b[38;5;241m=\u001b[39m pwd \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/imp/imp_score_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fl:\n",
      "File \u001b[0;32m~/Desktop/PHD/PHD COURSES/Parallel Computing HPC/HPC-Project-AD/ExIFFI/models/Extended_DIFFI_parallel.py:283\u001b[0m, in \u001b[0;36mExtended_DIFFI_parallel.Global_importance\u001b[0;34m(self, X, calculate, overwrite, depth_based)\u001b[0m\n\u001b[1;32m    280\u001b[0m anomaly_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAnomaly_Score(X)\n\u001b[1;32m    281\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(anomaly_scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)) :]\n\u001b[0;32m--> 283\u001b[0m importances_matrix, normal_vectors_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImportances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_based\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m Outliers_mean_importance_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(importances_matrix[ind], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    288\u001b[0m Inliers_mean_Importance_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    289\u001b[0m     importances_matrix[np\u001b[38;5;241m.\u001b[39mdelete(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(importances_matrix)), ind)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/PHD/PHD COURSES/Parallel Computing HPC/HPC-Project-AD/ExIFFI/models/Extended_DIFFI_parallel.py:220\u001b[0m, in \u001b[0;36mExtended_DIFFI_parallel.Importances\u001b[0;34m(self, X, calculate, overwrite, depth_based)\u001b[0m\n\u001b[1;32m    217\u001b[0m segment_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_processes\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# from this: [tree0, tree1, tree2, tree3, tree4]\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# to this: [  [tree0, tree1],   [tree2, tree3], [tree4]]]\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msegment_size\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m forest_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_forest_worker(X, depth_based)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# the result list of tuples which are the outputs of the make_importance function\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "y_train=np.zeros(X_train.shape[0])\n",
    "y_test=np.ones(X_test.shape[0])\n",
    "y=np.concatenate([y_train,y_test])\n",
    "X_test=np.r_[X_train,X_test]\n",
    "scaler2=StandardScaler()\n",
    "X=scaler2.fit_transform(X)\n",
    "EDIFFI=Extended_DIFFI_parallel(300,max_depth=100,subsample_size=256,plus=1)\n",
    "EDIFFI.set_num_processes(8)\n",
    "dim=X.shape[1]\n",
    "pwd=os.path.dirname(os.getcwd())\n",
    "start=time.time()\n",
    "imps,plt_data=compute_imps(EDIFFI,X,X,10,name,pwd,dim,f=6)\n",
    "end=time.time()\n",
    "print(f'Elapsed time: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((351, 33), (351, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y=load_data('ionosphere')\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (100000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y=load_data_csv('diabetes')\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
