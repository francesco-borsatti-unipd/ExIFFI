{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Host capri Davide\n",
    "\n",
    "    Hostname capri.dei.unipd.it\n",
    "    User p1026u27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "from utils.utils import partition_data\n",
    "from utils.feature_selection import *\n",
    "#from plot import *\n",
    "#from simulation_setup import *\n",
    "from models import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI_parallel import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path = os.path.dirname(path)\n",
    "path_real = os.path.join(path, \"data\", \"real\")\n",
    "mat_files_real = glob(os.path.join(path_real, \"*.mat\"))\n",
    "mat_file_names_real = {os.path.basename(x).split(\".\")[0]: x for x in mat_files_real}\n",
    "csv_files_real = glob(os.path.join(path_real, \"*.csv\"))\n",
    "csv_file_names_real = {os.path.basename(x).split(\".\")[0]: x for x in csv_files_real}\n",
    "dataset_names = list(mat_file_names_real.keys()) + list(csv_file_names_real.keys())\n",
    "mat_file_names_real.update(csv_file_names_real)\n",
    "dataset_paths = mat_file_names_real.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates from the loaded dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(X, y):\n",
    "    S = np.c_[X, y]\n",
    "    S = pd.DataFrame(S).drop_duplicates().to_numpy()\n",
    "    X, y = S[:, :-1], S[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset coming from a `.mat` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = loadmat(path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    y = np.hstack(y)\n",
    "    X, y = drop_duplicates(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset coming from a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_csv(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "    if \"Unnamed: 0\" in data.columns:\n",
    "        data = data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    X = data[data.columns[data.columns != \"Target\"]]\n",
    "    y = data[\"Target\"]\n",
    "\n",
    "    X, y = drop_duplicates(X, y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data (with `load_data` or with `load_data_csv`), scale the data and split it into train and test set obtaining `X_train`, `X_test` that will be passed to `compute_imps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(path):\n",
    "    extension = os.path.splitext(path)[1]\n",
    "\n",
    "    if extension == \".csv\":\n",
    "        X, y = load_data_csv(path)\n",
    "    elif extension == \".mat\":\n",
    "        X, y = load_data(path)\n",
    "    else:\n",
    "        raise ValueError(\"Extension not supported\")\n",
    "\n",
    "    X_train, X_test = partition_data(X, y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = np.r_[X_train, X_test]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Global Importance of a given dataset `n_runs` times. At the end a matrix with shape `(n_runs, n_features)` is returned. Each row contains the global importance of the features for a given run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imps(model, X_train, X_test, n_runs):\n",
    "\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "\n",
    "    imps = np.zeros(shape=(n_runs, X_train.shape[1]))\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        model.fit(X_train)\n",
    "        imps[i, :] = model.Global_importance(\n",
    "            X_test, calculate=True, overwrite=False, depth_based=False\n",
    "        )\n",
    "\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test_exiffi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function called in the `main` of `test_parallel.py` used to do the experiments on the CAPRI HPC server. For a given set of datasets it computes the global importance `n_runs` times using `Extended_DIFFI_parallel` or `Extended_DIFFI_original`and saves the importances matrices, the time stats obtained and the test arguments in a `.npz` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `test_exiffi` Parameters\n",
    "\n",
    "- `X_train`: the train set\n",
    "- `X_test`: the test set\n",
    "- `savedir`: directory where to save the results in `.npz` format\n",
    "- `n_runs`: number of runs to do\n",
    "- `seed`: random seed to obtain reproducibile results and compare the importances matrices obtaind from the parallel and the serial version of the algorithm (they must be the same to certify the correctness of the parallel version)\n",
    "- `parallel`: Boolean variable used to choose between the parallel and the serial version of the algorithm\n",
    "- `n_cores`: Number of threads to use in the parallel version of the algorithm. This coincides with the number of cores set with the `--cpus-per-task` options in the `.job` file\n",
    "- `num_trees`: Number of trees used by ExIFFI. The higher the more complex and more computationally expensive the algorithm is\n",
    "- `name`: Name of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_exiffi(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    savedir,\n",
    "    n_runs=10,\n",
    "    seed=None,\n",
    "    parallel=False,\n",
    "    n_cores=2,\n",
    "    num_trees=300,\n",
    "    name=\"\",\n",
    "):\n",
    "    args_to_avoid = [\"X_train\", \"X_test\", \"savedir\", \"args_to_avoid\", \"args\"]\n",
    "    args = dict()\n",
    "    for k, v in locals().items():\n",
    "        if k in args_to_avoid:\n",
    "            continue\n",
    "        args[k] = v\n",
    "\n",
    "    ex_time = []\n",
    "    ex_imps = {}\n",
    "\n",
    "    for i in trange(n_runs):\n",
    "        seed = None if seed is None else seed + i\n",
    "\n",
    "        if parallel:\n",
    "            EDIFFI = Extended_DIFFI_parallel(\n",
    "                n_trees=num_trees, max_depth=100, subsample_size=256, plus=1, seed=seed\n",
    "            )\n",
    "            EDIFFI.set_num_processes(n_cores, n_cores)\n",
    "        else:\n",
    "            EDIFFI = Extended_DIFFI_original(\n",
    "                n_trees=num_trees, max_depth=100, subsample_size=256, plus=1, seed=seed\n",
    "            )\n",
    "\n",
    "        start = time.time()\n",
    "        imps = compute_imps(EDIFFI, X_train, X_test, 10)\n",
    "        ex_imps[\"Execution \" + str(i)] = imps\n",
    "        end = time.time()\n",
    "        ex_time.append(end - start)\n",
    "\n",
    "    # print(ex_imps)\n",
    "    time_stat = {\"mean\": np.mean(ex_time), \"std\": np.std(ex_time)}\n",
    "    filename = \"test_stat_parallel.npz\" if parallel else \"test_stat_serial.npz\"\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%d-%m-%Y_%H-%M-%S\", t)\n",
    "    filename = current_time + \"_\" + name + \"_\" + filename\n",
    "\n",
    "    # if dir does not exist, create it\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    filepath = os.path.join(savedir, filename)\n",
    "\n",
    "    np.savez(\n",
    "        filepath,\n",
    "        execution_time_stat=time_stat,\n",
    "        importances_matrix=ex_imps,\n",
    "        arguments=args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129, 13), (129,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='wine'\n",
    "X,y=load_data(dataset_paths[name])\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=False,\n",
    "    n_cores=12,\n",
    "    num_trees=10,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=True,\n",
    "    n_cores=12,\n",
    "    num_trees=200,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((350, 33), (350,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='ionosphere'\n",
    "X,y=load_data(dataset_paths[name])\n",
    "X_train,X_test=partition_data(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=False,\n",
    "    n_cores=12,\n",
    "    num_trees=10,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel ExIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exiffi(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    savedir='../results/npz',\n",
    "    n_runs=1,\n",
    "    seed=120,\n",
    "    parallel=True,\n",
    "    n_cores=12,\n",
    "    num_trees=200,\n",
    "    name=name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=np.load('26-01-2024_17-42-35_test_stat_parallel_7000.npz',allow_pickle=True)\n",
    "data_parallel=stats['importances_matrix'].tolist()\n",
    "time_data_parallel=stats['execution_time_stat']\n",
    "arguments_parallel=stats['arguments'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'X', 'n_runs', 'seed', 'parallel', 'n_cores'])\n",
      "n_runs 2\n",
      "seed None\n",
      "parallel True\n",
      "n_cores 8\n"
     ]
    }
   ],
   "source": [
    "print(arguments_parallel.keys())\n",
    "\n",
    "args_to_avoid = [\"X_train\", \"X_test\", \"X\"]\n",
    "for key in arguments_parallel.keys():\n",
    "    if key not in args_to_avoid:\n",
    "        print(key,arguments_parallel[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.36362202167511, 'std': 0.23129910849918273}, dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parallel.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_stat_serial.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_stat_serial.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_serial\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportances_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m time_data_serial\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_time_stat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_stat_serial.npz'"
     ]
    }
   ],
   "source": [
    "stats=np.load('test_stat_serial.npz',allow_pickle=True)\n",
    "data_serial=stats['importances_matrix'].tolist()\n",
    "time_data_serial=stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 3.5636572360992433, 'std': 0.5714168745774968},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Execution 0', 'Execution 1', 'Execution 2', 'Execution 3', 'Execution 4', 'Execution 5', 'Execution 6', 'Execution 7', 'Execution 8', 'Execution 9'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_serial.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if `data_parallel` and `data_serial` are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.995204332975845e-14\n",
      "1.532107773982716e-13\n",
      "-3.4638958368304884e-13\n",
      "7.327471962526033e-14\n",
      "5.306866057708248e-13\n",
      "-3.774758283725532e-13\n",
      "-3.197442310920451e-13\n",
      "1.3522516439934407e-12\n",
      "-1.2434497875801753e-13\n",
      "-1.0769163338864018e-12\n"
     ]
    }
   ],
   "source": [
    "for k in data_serial.keys():\n",
    "    print(np.sum(data_serial[k]-data_parallel[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Thyroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 89.01771640777588, 'std': 0.0}, dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_runs': 1,\n",
       " 'seed': 120,\n",
       " 'parallel': True,\n",
       " 'n_cores': 2,\n",
       " 'num_trees': 10,\n",
       " 'name': 'annthyroid',\n",
       " 'args_to_avoid': ['X_train', 'X_test', 'savedir'],\n",
       " 'args': {...}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_load = (\n",
    "    \"../capri_code/results/npz/28-01-2024_17-45-18_annthyroid_test_stat_parallel.npz\"\n",
    ")\n",
    "\n",
    "stats = np.load(path_to_load, allow_pickle=True)\n",
    "\n",
    "display(stats['execution_time_stat'])\n",
    "display(stats['arguments'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'mean': 22.09242186546326, 'std': 2.2678216673392386}, dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_stats['execution_time_stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'n_runs': 10, 'seed': 120, 'parallel': True, 'n_cores': 12, 'num_trees': 300, 'name': 'wine', 'args_to_avoid': ['X_train', 'X_test', 'savedir'], 'args': {...}},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_stats['arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484],\n",
       "       [1.51150254, 0.94012808, 0.77765704, 0.85334869, 0.92756969,\n",
       "        1.01631053, 1.27235724, 1.37869805, 0.93163375, 1.2656863 ,\n",
       "        1.18378956, 1.39478296, 1.38035484]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_mat_wine=wine_stats['importances_matrix'].tolist()\n",
    "imp_mat_wine['Execution 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All results\n",
    "\n",
    "We use the script `process_results.py` to read the stats of the experiments from the `.npz` files and display them on a `pd.DataFrame` that can be saved as a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastw\n"
     ]
    }
   ],
   "source": [
    "for data in stats:\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_runs</th>\n",
       "      <th>seed</th>\n",
       "      <th>parallel</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>n_trees</th>\n",
       "      <th>name</th>\n",
       "      <th>mean_time</th>\n",
       "      <th>std_time</th>\n",
       "      <th>mean_MB</th>\n",
       "      <th>std_MB</th>\n",
       "      <th>max_MB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>glass</td>\n",
       "      <td>9.483155</td>\n",
       "      <td>0.089877</td>\n",
       "      <td>338.409062</td>\n",
       "      <td>1.062795</td>\n",
       "      <td>338.743296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_runs  seed  parallel  n_cores  n_trees   name  mean_time  std_time  \\\n",
       "0       2  None      True        7       10  glass   9.483155  0.089877   \n",
       "\n",
       "      mean_MB    std_MB      max_MB  \n",
       "0  338.409062  1.062795  338.743296  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "\n",
    "from capri_code.process_results import load_stats, display_stats\n",
    "\n",
    "\n",
    "results_dirpath = \"../capri_code/results/npz/\"\n",
    "\n",
    "stats = load_stats(results_dirpath)\n",
    "\n",
    "display_stats(stats)\n",
    "# display_stats(stats.groupby(\"name\").get_group(\"cardio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importances_matrix</th>\n",
       "      <th>n_runs</th>\n",
       "      <th>seed</th>\n",
       "      <th>parallel</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>n_trees</th>\n",
       "      <th>name</th>\n",
       "      <th>mean_time</th>\n",
       "      <th>std_time</th>\n",
       "      <th>mean_MB</th>\n",
       "      <th>std_MB</th>\n",
       "      <th>max_MB</th>\n",
       "      <th>n_runs_imps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.8197615431146767, 0.9772933701175461, 0.8...</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>glass</td>\n",
       "      <td>73.373395</td>\n",
       "      <td>0.336610</td>\n",
       "      <td>415.168922</td>\n",
       "      <td>12.585211</td>\n",
       "      <td>431.443968</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[1.6035938204969806, 1.6317114425255999, 1.6...</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>breastw</td>\n",
       "      <td>23.554641</td>\n",
       "      <td>1.770169</td>\n",
       "      <td>513.024000</td>\n",
       "      <td>25.022464</td>\n",
       "      <td>538.046464</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  importances_matrix  n_runs  seed  parallel  \\\n",
       "0  [[[0.8197615431146767, 0.9772933701175461, 0.8...       2   123      True   \n",
       "1  [[[1.6035938204969806, 1.6317114425255999, 1.6...       2   123      True   \n",
       "\n",
       "   n_cores  n_trees     name  mean_time  std_time     mean_MB     std_MB  \\\n",
       "0        7      100    glass  73.373395  0.336610  415.168922  12.585211   \n",
       "1        7      300  breastw  23.554641  1.770169  513.024000  25.022464   \n",
       "\n",
       "       max_MB  n_runs_imps  \n",
       "0  431.443968          NaN  \n",
       "1  538.046464          1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imps_mat.shape (2, 10, 9)\n",
      "imp_mat_ex_0.shape (10, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from append_dir import append_dirname\n",
    "append_dirname('ExIFFI')\n",
    "\n",
    "from capri_code.process_results import load_stats, display_stats\n",
    "\n",
    "\n",
    "results_dirpath = \"../capri_code/results/npz/new/new\"\n",
    "\n",
    "stats = load_stats(results_dirpath)\n",
    "\n",
    "# display_stats(stats)\n",
    "display(stats)\n",
    "\n",
    "imps_mat = np.array(stats.loc[0, \"importances_matrix\"])\n",
    "\n",
    "print(\"imps_mat.shape\", imps_mat.shape)\n",
    "\n",
    "\n",
    "imp_mat_ex_0 = imps_mat[0]\n",
    "print(\"imp_mat_ex_0.shape\", imp_mat_ex_0.shape)\n",
    "\n",
    "# for i in range(len(imp_mat_ex_0)-1):\n",
    "#     print(imp_mat_ex_0[i] - imp_mat_ex_0[i+1])\n",
    "\n",
    "imp_mat_ex_1 = imps_mat[1]\n",
    "\n",
    "# for i in range(len(imp_mat_ex_1)):\n",
    "#     print(imp_mat_ex_1[i] - imp_mat_ex_0[i])\n",
    "\n",
    "# print(imps_mat[0] - imps_mat[1])\n",
    "# print(imps_mat[1] - imps_mat[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "(3, 1)\n",
      "output\n",
      " [[11.]\n",
      " [11.]\n",
      " [11.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return x+10\n",
    "\n",
    "a = np.ones((3))\n",
    "\n",
    "# add newaxis to a\n",
    "a = a[:, np.newaxis]\n",
    "\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "output = np.apply_along_axis(func, 1, a)\n",
    "\n",
    "print(\"output\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
